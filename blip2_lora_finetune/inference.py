# import os
# import json
# import torch
# from tqdm import tqdm
# from PIL import Image
# from transformers import Blip2Processor, Blip2ForConditionalGeneration
# from peft import PeftModel

# # ==== é…ç½® ====
# base_model_id = "blip2-opt-2.7b"  # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ID
# lora_checkpoint = "checkpoints/checkpoint-3750"  # åŒ…å« adapter_config.json å’Œ adapter_model.safetensors
# test_json_path = "blip2_lora_finetune/flickr8k_test.json"
# image_dir = "/root/autodl-tmp/data/Flickr8k/Images"
# output_json_path = "generated_captions.json"
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # ==== åŠ è½½æ¨¡å‹å’Œ processor ====
# print("ğŸ”§ åŠ è½½æ¨¡å‹...")
# processor = Blip2Processor.from_pretrained(base_model_id)

# # åŠ è½½åŸºç¡€æ¨¡å‹
# base_model = Blip2ForConditionalGeneration.from_pretrained(
#     base_model_id,
#     device_map="auto",
#     load_in_8bit=True,
#     torch_dtype=torch.float16  # ä½¿ç”¨åŠç²¾åº¦å‡å°‘æ˜¾å­˜å ç”¨
# )

# # åŠ è½½LoRAé€‚é…å™¨
# try:
#     model = PeftModel.from_pretrained(base_model, lora_checkpoint)
#     print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
#     # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
#     model = model.merge_and_unload()
#     print("âœ… LoRAæƒé‡å·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹")
# except Exception as e:
#     print(f"âš ï¸ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥: {e}")
#     print("âš ï¸ ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†")
#     model = base_model

# model.eval()
# print("âœ… æ¨¡å‹å‡†å¤‡å°±ç»ª")

# # ==== åŠ è½½æµ‹è¯•é›† ====
# print("ğŸ“‚ åŠ è½½æµ‹è¯•é›†...")
# with open(test_json_path, "r") as f:
#     test_data = json.load(f)

# results = []

# # # ==== ä¼˜åŒ–åçš„æç¤ºå·¥ç¨‹ ====
# # def create_prompt(image_path):
# #     """æ ¹æ®å›¾åƒè·¯å¾„åˆ›å»ºæ›´è‡ªç„¶çš„æç¤º"""
# #     filename = os.path.basename(image_path)
# #     return f"Describe the content of the image '{filename}' in a detailed and complete sentence."

# # ==== éå†æµ‹è¯•å›¾åƒå¹¶ç”Ÿæˆæè¿° ====
# print("ğŸ§  å¼€å§‹ç”Ÿæˆæè¿°...")
# for item in tqdm(test_data):
#     image_path = os.path.join(image_dir, item["image"])
    
#     try:
#         image = Image.open(image_path).convert("RGB")
#     except Exception as e:
#         print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒ {image_path}: {e}")
#         results.append({
#             "image": item["image"],
#             "caption": "[IMAGE LOAD ERROR]",
#             "error": str(e)
#         })
#         continue

#     # åˆ›å»ºæ›´è‡ªç„¶çš„æç¤º
#     prompt = 'Write a caption for this image.'
    
#     try:
#         # å¤„ç†è¾“å…¥
#         inputs = processor(
#             images=image, 
#             text=prompt, 
#             return_tensors="pt"
#         ).to(device, torch.float16)
        
#         # ç”Ÿæˆæè¿°
#         with torch.no_grad():
#             generated_ids = model.generate(
#                 input_ids=inputs["input_ids"],
#                 pixel_values=inputs["pixel_values"],
#                 attention_mask=inputs["attention_mask"],
#                 max_new_tokens=50,
#                 num_beams=6,  
#                 early_stopping=True,
#                 no_repeat_ngram_size=3,
#                 eos_token_id=model.config.eos_token_id,
#                 )
        
#         # è§£ç å¹¶æ¸…ç†è¾“å‡º
#         caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
        
#         # ç§»é™¤æç¤ºæ–‡æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#         if prompt.lower() in caption.lower():
#             idx = caption.lower().index(prompt.lower())
#             caption = caption[idx + len(prompt):].strip()
        
#         # æ¸…ç†æ ‡ç‚¹ç¬¦å·
#         caption = caption.strip().strip('"').strip()
#         if caption and caption[0] in ['.', ',', ':', ';']:
#             caption = caption[1:].strip()
        
#         # ç¡®ä¿é¦–å­—æ¯å¤§å†™
#         if caption:
#             caption = caption[0].upper() + caption[1:]
        
#     except Exception as e:
#         print(f"âš ï¸ ç”Ÿæˆæè¿°å¤±è´¥: {e}")
#         caption = f"[GENERATION ERROR: {str(e)}]"
    
#     # å®æ—¶æ‰“å°ç”Ÿæˆçš„æè¿°
#     print(f"[{item['image']}] => \"{caption}\"")
    
#     results.append({
#         "image": item["image"],
#         "caption": caption
#     })

# # ==== ä¿å­˜ç»“æœ ====
# with open(output_json_path, "w") as f:
#     json.dump(results, f, indent=2, ensure_ascii=False)

# print(f"âœ… æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š{output_json_path}")
# print(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒæ•°é‡: {len(results)}")

import os
import json
import logging
import torch
from tqdm import tqdm
from PIL import Image
from transformers import Blip2Processor, Blip2ForConditionalGeneration, GPT2Tokenizer
from transformers import GPT2TokenizerFast
from peft import PeftModel, PeftConfig
from transformers import AutoTokenizer
# ==== é…ç½®æ—¥å¿— ====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("inference.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==== é…ç½®å‚æ•° ====
class Config:
    # æ¨¡å‹é…ç½®
    base_model_id = "blip2-opt-2.7b"
    lora_checkpoint = "checkpoint/checkpoint-3750"
    
    # æ•°æ®é…ç½®
    test_json_path = "blip2_lora_finetune/flickr8k_test.json"
    image_dir = "/root/autodl-tmp/data/Flickr8k/Images"
    output_json_path = "generated_captions.json"
    
    # æ¨ç†å‚æ•°
    max_new_tokens = 100
    num_beams = 5
    temperature = 0.7
    top_p = 0.9
    repetition_penalty = 1.5
    no_repeat_ngram_size = 2
    
    # ç³»ç»Ÿå‚æ•°
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch_dtype = torch.float16
    validate_tokenizer = True

config = Config()

# ==== éªŒè¯åˆ†è¯å™¨ä¸æ¨¡å‹çš„å…¼å®¹æ€§ ====
def validate_tokenizer(model, processor):
    """å…¨é¢éªŒè¯åˆ†è¯å™¨ä¸æ¨¡å‹çš„å…¼å®¹æ€§"""
    logger.info("="*50)
    logger.info("å¼€å§‹åˆ†è¯å™¨éªŒè¯")
    logger.info("="*50)
    
    text_config = model.config.text_config
    tokenizer = processor.tokenizer
    
    # 1. åŸºæœ¬ä¿¡æ¯æ£€æŸ¥
    tokenizer_type = type(tokenizer).__name__
    model_lm_type = text_config.model_type
    
    logger.info(f"Tokenizer ç±»å‹: {tokenizer_type}")
    logger.info(f"æ¨¡å‹è¯­è¨€æ¨¡å‹ç±»å‹: {model_lm_type}")
    logger.info(f"Tokenizer è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    logger.info(f"æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {text_config.vocab_size}")
    
    # æ£€æŸ¥ç±»å‹åŒ¹é…
    if tokenizer_type.lower() != model_lm_type.lower():
        logger.warning(f"âš ï¸ ä¸¥é‡ä¸åŒ¹é…: Tokenizerç±»å‹({tokenizer_type}) != æ¨¡å‹ç±»å‹({model_lm_type})")
    else:
        logger.info("âœ… Tokenizerç±»å‹åŒ¹é…")
    
    # æ£€æŸ¥è¯æ±‡è¡¨å¤§å°
    if tokenizer.vocab_size != text_config.vocab_size:
        logger.warning(f"âš ï¸ ä¸¥é‡ä¸åŒ¹é…: Tokenizerè¯æ±‡è¡¨å¤§å°({tokenizer.vocab_size}) != æ¨¡å‹è¯æ±‡è¡¨å¤§å°({text_config.vocab_size})")
    else:
        logger.info("âœ… è¯æ±‡è¡¨å¤§å°åŒ¹é…")
    
    # 2. ç‰¹æ®Šæ ‡è®°æ£€æŸ¥
    logger.info("\nç‰¹æ®Šæ ‡è®°æ£€æŸ¥:")
    tokens_to_check = {
        "eos_token": (tokenizer.eos_token_id, text_config.eos_token_id),
        "bos_token": (tokenizer.bos_token_id, text_config.bos_token_id),
        "pad_token": (tokenizer.pad_token_id, text_config.pad_token_id),
        "unk_token": (tokenizer.unk_token_id, getattr(text_config, "unk_token_id", None))
    }
    
    all_match = True
    for name, (tokenizer_id, model_id) in tokens_to_check.items():
        if model_id is None:
            logger.info(f"{name}: æ¨¡å‹æœªå®šä¹‰")
            continue
            
        if tokenizer_id == model_id:
            logger.info(f"âœ… {name}: IDåŒ¹é… ({tokenizer_id})")
        else:
            logger.warning(f"âš ï¸ {name}: ä¸åŒ¹é…! Tokenizer={tokenizer_id}, æ¨¡å‹={model_id}")
            all_match = False
    
    # 3. ç¼–ç /è§£ç æµ‹è¯•
    logger.info("\nç¼–ç /è§£ç æµ‹è¯•:")
    test_texts = [
        "A cat sitting on a mat",
        "Describe this image in detail:",
        "å›¾åƒæè¿°ï¼šä¸€åªç‹—åœ¨å…¬å›­é‡Œå¥”è·‘",
        tokenizer.eos_token if tokenizer.eos_token else "<|endoftext|>",
        "Special !@#$% characters"
    ]
    
    for text in test_texts:
        try:
            encoded = tokenizer.encode(text)
            decoded = tokenizer.decode(encoded, skip_special_tokens=True)
            
            if decoded == text:
                logger.info(f"âœ… '{text}' -> ç¼–ç /è§£ç ä¸€è‡´")
            else:
                logger.warning(f"âš ï¸ '{text}' -> è§£ç ä¸º '{decoded}'")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç† '{text}' æ—¶å‡ºé”™: {str(e)}")
    
    # 4. åµŒå…¥å±‚æ£€æŸ¥
    try:
        logger.info("\nåµŒå…¥å±‚æ£€æŸ¥:")
        embedding_layer = model.get_input_embeddings()
        logger.info(f"åµŒå…¥å±‚å¤§å°: {embedding_layer.num_embeddings}")
        
        # æµ‹è¯•è¾¹ç•Œtoken
        test_tokens = [0, 1, tokenizer.vocab_size - 1, text_config.vocab_size - 1]
        for token_id in test_tokens:
            try:
                embedding = embedding_layer(torch.tensor([token_id]).to(config.device))
                logger.info(f"Token {token_id} åµŒå…¥æˆåŠŸ (å½¢çŠ¶: {embedding.shape})")
            except IndexError:
                logger.error(f"âŒ Token {token_id} è¶…å‡ºåµŒå…¥å±‚èŒƒå›´!")
                
    except Exception as e:
        logger.error(f"æ— æ³•è®¿é—®åµŒå…¥å±‚: {str(e)}")
    
    logger.info("="*50)
    logger.info("åˆ†è¯å™¨éªŒè¯å®Œæˆ")
    logger.info("="*50)
    
    return all_match

# ==== åŠ è½½æ¨¡å‹ ====
def load_model():
    """åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨"""
    logger.info("ğŸ”§ åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½å¤„ç†å™¨ - æ˜¾å¼æŒ‡å®šOPTåˆ†è¯å™¨
    try:
        processor = Blip2Processor.from_pretrained("checkpoint/checkpoint-3750/processor")
        # opt_tokenizer = AutoTokenizer.from_pretrained("opt-2.7b", use_fast=False)
        # processor.tokenizer = opt_tokenizer  # å¼ºåˆ¶æ›¿æ¢ä¸ºæ­£ç¡® tokenizer
        logger.info(f"âœ… åŠ è½½å¤„ç†å™¨å®Œæˆ: {type(processor).__name__}")
    except Exception as e:
        logger.error(f"âŒ åŠ è½½å¤„ç†å™¨å¤±è´¥: {str(e)}")
        raise
        
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    try:
        base_model = Blip2ForConditionalGeneration.from_pretrained(
            config.base_model_id,
            device_map="auto",
            load_in_8bit=True,
            torch_dtype=config.torch_dtype
        )
        logger.info(f"âœ… åŠ è½½åŸºç¡€æ¨¡å‹å®Œæˆ: {base_model.__class__.__name__}")
    except Exception as e:
        logger.error(f"âŒ åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {str(e)}")
        raise
    
    # å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨å¯¹é½ç‰¹æ®Šæ ‡è®°
    base_model.config.text_config.eos_token_id = processor.tokenizer.eos_token_id
    base_model.config.text_config.pad_token_id = processor.tokenizer.pad_token_id
    base_model.config.text_config.bos_token_id = processor.tokenizer.bos_token_id
    
    logger.info(f"âœ… æ‰‹åŠ¨å¯¹é½ç‰¹æ®Šæ ‡è®°: "
                f"EOS={base_model.config.text_config.eos_token_id}, "
                f"PAD={base_model.config.text_config.pad_token_id}, "
                f"BOS={base_model.config.text_config.bos_token_id}")
    
    # åŠ è½½LoRAé€‚é…å™¨
    model = base_model
    if os.path.exists(config.lora_checkpoint):
        try:
            # æ£€æŸ¥LoRAé…ç½®
            peft_config = PeftConfig.from_pretrained(config.lora_checkpoint)
            logger.info(f"LoRAé…ç½®: {peft_config.to_dict()}")
            
            # åŠ è½½é€‚é…å™¨
            model = PeftModel.from_pretrained(base_model, config.lora_checkpoint)
            logger.info("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
            
            # åˆå¹¶æƒé‡
            model = model.merge_and_unload()
            logger.info("âœ… LoRAæƒé‡å·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹")
        except Exception as e:
            logger.error(f"âš ï¸ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥: {str(e)}")
            logger.warning("âš ï¸ ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†")
    else:
        logger.warning(f"âš ï¸ LoRAæ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {config.lora_checkpoint}")
        logger.warning("âš ï¸ ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†")
    
    # éªŒè¯åˆ†è¯å™¨
    if config.validate_tokenizer:
        logger.info("ğŸ” éªŒè¯åˆ†è¯å™¨å…¼å®¹æ€§...")
        tokenizer_valid = validate_tokenizer(model, processor)
        if not tokenizer_valid:
            logger.warning("âš ï¸ åˆ†è¯å™¨éªŒè¯å‘ç°æ½œåœ¨é—®é¢˜ï¼Œæ¨ç†ç»“æœå¯èƒ½å—å½±å“")
    
    model.eval()
    logger.info("âœ… æ¨¡å‹å‡†å¤‡å°±ç»ª")
    
    return processor, model

# ==== åˆ›å»ºæç¤º ====
def create_prompt(image_path):
    """æ ¹æ®å›¾åƒè·¯å¾„åˆ›å»ºæç¤º"""
    filename = os.path.basename(image_path)
    return f"A detailed description of the image '{filename}':"

# ==== æ¸…ç†ç”Ÿæˆçš„æè¿° ====
def clean_caption(caption, prompt):
    """æ¸…ç†ç”Ÿæˆçš„æè¿°æ–‡æœ¬"""
    # ç§»é™¤æç¤ºæ–‡æœ¬
    prompt_lower = prompt.lower()
    caption_lower = caption.lower()
    
    if prompt_lower in caption_lower:
        idx = caption_lower.index(prompt_lower)
        caption = caption[idx + len(prompt):].strip()
    
    # æ¸…ç†å¼€å¤´æ ‡ç‚¹
    while caption and caption[0] in ['.', ',', ':', ';', '-', 'â€”']:
        caption = caption[1:].strip()
    
    # ç¡®ä¿é¦–å­—æ¯å¤§å†™
    if caption:
        caption = caption[0].upper() + caption[1:]
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½
    caption = ' '.join(caption.split())
    logger.info(f"âœ… caption: {caption} ")
    return caption

# ==== ä¸»å‡½æ•° ====
def main():
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    processor, model = load_model()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    logger.info("ğŸ“‚ åŠ è½½æµ‹è¯•é›†...")
    try:
        with open(config.test_json_path, "r") as f:
            test_data = json.load(f)
        logger.info(f"âœ… åŠ è½½æµ‹è¯•é›†å®Œæˆ: {len(test_data)} ä¸ªæ ·æœ¬")
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æµ‹è¯•é›†å¤±è´¥: {str(e)}")
        return
    
    results = []
    error_count = 0
    success_count = 0
    
    # æ¨ç†å¾ªç¯
    logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆæè¿°...")
    for item in tqdm(test_data, desc="ç”Ÿæˆæè¿°"):
        image_path = os.path.join(config.image_dir, item["image"])
        result_item = {
            "image": item["image"],
            "caption": "",
            "error": None
        }
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            
            # åˆ›å»ºæç¤º
            prompt = create_prompt(item["image"])
            
            # é¢„å¤„ç†è¾“å…¥
            inputs = processor(
                images=image, 
                text=prompt, 
                return_tensors="pt"
            ).to(config.device, config.torch_dtype)
            
            # ç”Ÿæˆæè¿° - å…³é”®ä¿®å¤ï¼šä½¿ç”¨åˆ†è¯å™¨çš„EOS token
            with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=config.torch_dtype):
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=config.max_new_tokens,
                    num_beams=config.num_beams,
                    repetition_penalty=config.repetition_penalty,
                    no_repeat_ngram_size=config.no_repeat_ngram_size,
                    eos_token_id=processor.tokenizer.eos_token_id,  # å…³é”®ä¿®å¤
                    early_stopping=True
                )
            
            # è§£ç è¾“å‡º
            caption = processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0].strip()
            
            # æ¸…ç†æè¿°
            caption = clean_caption(caption, prompt)
            result_item["caption"] = caption
            success_count += 1
            
            # å®æ—¶æ—¥å¿—
            if success_count % 50 == 0 or success_count == 1:
                logger.info(f"ğŸ–¼ï¸ [{item['image']}] => \"{caption}\"")
            
        except Exception as e:
            error_msg = f"å¤„ç† {item['image']} æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg)
            result_item["error"] = error_msg
            error_count += 1
        
        results.append(result_item)
    
    # ä¿å­˜ç»“æœ
    try:
        with open(config.output_json_path, "w") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {config.output_json_path}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(test_data)} å¼ å›¾åƒ")
    logger.info(f"âœ… æˆåŠŸ: {success_count} ({(success_count/len(test_data))*100:.2f}%)")
    logger.info(f"âŒ å¤±è´¥: {error_count} ({(error_count/len(test_data))*100:.2f}%)")
    
    # ä¿å­˜é”™è¯¯æ—¥å¿—
    if error_count > 0:
        error_log_path = "inference_errors.json"
        errors = [r for r in results if r["error"]]
        with open(error_log_path, "w") as f:
            json.dump(errors, f, indent=2)
        logger.info(f"âš ï¸ é”™è¯¯è¯¦æƒ…å·²ä¿å­˜è‡³: {error_log_path}")

if __name__ == "__main__":
    main()